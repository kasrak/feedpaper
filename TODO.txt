- run enrichment script automatically
- bootstrap sqlite db schema and prepare for migrations (https://github.com/kriasoft/node-sqlite)
- don't pull focus when app runs scraping
- click to expand media
- animated gifs/videos
- "show this thread"?
- open tweets in dedicated twitter tab (and separate from bg fetch twitter tab)
- omit plain retweets if original tweet (or a retweet of it) was in yesterday's digest
- if last link is to quoted/retweeted tweet, remove it from text: http://localhost:3000/tweet/1641563370431164416
- if the tweet is just a link to another tweet, don't show the link in text: http://localhost:3000/tweet/1646368688189186054
- "show more" for note_tweets hides media (extended_entities?): http://localhost:3000/tweet/1641482810266648576
- improve sorting, e.g. http://localhost:3000/?date=2023-04-13 "databricks"
- merge related clusters (fix greedy bug)
- use GPT to summarize clusters with 3+ tweets
- pull in original tweet for replies, e.g. https://twitter.com/ntkris/status/1641585664654295041?s=20
- remember scroll position per day **
- dark mode

- demo notes
    - first: 24-hour tape delay
        - solves fomo, constantly checking. everything will be there tomorrow,
        and there's no incentive to check multiple times per day.
        - but also lets us do better grouping of tweets.
    - better grouping of tweets:
        - first: start with the data that's available: replies, retweets, quotes, urls.
          this creates "conversation" clusters.
        - then use gpt-3-turbo to extract entities, and re-sort the timeline so related
          conversation clusters are next to each other. a much better reading experience,
          because you're not constantly context switching between topics. transitions between
          related topics instead.
            - using this as a "continuous" sorting signal instead of a "discrete" grouping
              signal works better. sorting is more forgiving to the LLM getting it wrong.
    - the 24-hour tape delay also makes it possible to do all this post-processing without
      worrying about LLM speed, which is nice.
    - gpt-4 is better, but too expensive to feed every tweet through... for me that would be
      $60 per month. gpt-3-turbo is ~$3 per month. scales with number of people you follow and
      how much they tweet.
    - ideas for improvement
        - hide low-signal tweets based on your preferences, or rank based on relevance
          to your preferences
            - learn preferences based on interactions (e.g. clicks), use that to update
          prompt
        - summarize conversation clusters or larger sections, then decide if you want
          to go in and read all the tweets or not
        - pull in from other sources, e.g. HN, mastodon, etc
        - somehow visualize the topic space so you can "skip ahead" more quickly
        - better few-shot examples: for each prompt pick from list of examples based
          on semantic similarity to items that will be inside that prompt